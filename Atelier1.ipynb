{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdlkrim3/Natural-Language-Processing-NLP/blob/main/Atelier1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbm3wKP3fRLh",
        "outputId": "6064c6c3-8115-46b0-a36d-751fae631232"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TXgBX_ZfRLl"
      },
      "source": [
        "Pretraitement de texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JTWSsGcfRLn"
      },
      "outputs": [],
      "source": [
        "d1=\"Le chat dort sur le tapis\"\n",
        "d2=\"Les Oiseaux Chantent Le Matin\"\n",
        "d3=\"Le chien court dans le jardin\"\n",
        "d4=\"Mangeons des pommes délicieuses\"\n",
        "d5=\"Je mange une orange fraiche\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZkH6uH_fRLo"
      },
      "source": [
        "1. Crée un corpus qui contient les textes d1, d2, d3, d4, d5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZn3s4oZfRLp"
      },
      "outputs": [],
      "source": [
        "# corpus\n",
        "corpus = [\n",
        "    \"Le chat dort sur le tapis\",\n",
        "    \"Les Oiseaux Chantent Le Matin\",\n",
        "    \"Le chien court dans le jardin\",\n",
        "    \"Mangeons des pommes delicieuses\",\n",
        "    \"Je mange une orange fraiche\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3kCL8oyfRLp"
      },
      "source": [
        "2. Convertir le corpus en type DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE9wO-BCfRLq"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'Texte': corpus})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvdKRlWTfRLr",
        "outputId": "30422d45-6834-416b-8d82-74c8bb0a8930"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Le chat dort sur le tapis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Les Oiseaux Chantent Le Matin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Le chien court dans le jardin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mangeons des pommes delicieuses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Je mange une orange fraiche</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Texte\n",
              "0        Le chat dort sur le tapis\n",
              "1    Les Oiseaux Chantent Le Matin\n",
              "2    Le chien court dans le jardin\n",
              "3  Mangeons des pommes delicieuses\n",
              "4      Je mange une orange fraiche"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BGqrjjfRLs"
      },
      "source": [
        "3. Quelles sont les différentes approches pour prétraiter un corpus de textes? Décrire les."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHgXuFbTfRLt"
      },
      "source": [
        "4. Décrire le code pour la ponctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VjpFmF5fRLt",
        "outputId": "0c3f35e1-0fed-4ea5-d658-bad8b0144f27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "# Fonction pour supprimer la ponctuation\n",
        "print(string.punctuation)\n",
        "def code(texte):\n",
        "    text_s_pon=[c for c in texte if c not in string.punctuation]\n",
        "    return ''.join(text_s_pon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI8aLFpEfRLt"
      },
      "source": [
        "5. Ajouter une colonne dans l’objet corpus nommée « t_s_p » en utilisant la fonction de Q.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7YhogR3fRLu",
        "outputId": "d57fe2f2-b66e-4e7f-c663-eb6cf2a656cc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>t_s_p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Le chat dort sur le tapis</td>\n",
              "      <td>Le chat dort sur le tapis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Les Oiseaux Chantent Le Matin</td>\n",
              "      <td>Les Oiseaux Chantent Le Matin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Le chien court dans le jardin</td>\n",
              "      <td>Le chien court dans le jardin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mangeons des pommes delicieuses</td>\n",
              "      <td>Mangeons des pommes delicieuses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Je mange une orange fraiche</td>\n",
              "      <td>Je mange une orange fraiche</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Texte                            t_s_p\n",
              "0        Le chat dort sur le tapis        Le chat dort sur le tapis\n",
              "1    Les Oiseaux Chantent Le Matin    Les Oiseaux Chantent Le Matin\n",
              "2    Le chien court dans le jardin    Le chien court dans le jardin\n",
              "3  Mangeons des pommes delicieuses  Mangeons des pommes delicieuses\n",
              "4      Je mange une orange fraiche      Je mange une orange fraiche"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Appliquer la fonction pour créer la colonne \"t_s_p\"\n",
        "df['t_s_p'] = df['Texte'].apply(code)\n",
        "\n",
        "# Afficher le DataFrame\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x80XaPO_fRLu"
      },
      "source": [
        "6. Ecrire une fonction pour tokinezer le corpus de colonne « t_s_p »."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMXcMnfRfRLu",
        "outputId": "8b06d5a8-488b-4449-b44c-287f5ddbbe5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             Texte                                tokens\n",
            "0        Le chat dort sur le tapis      [Le, chat, dort, sur, le, tapis]\n",
            "1    Les Oiseaux Chantent Le Matin   [Les, Oiseaux, Chantent, Le, Matin]\n",
            "2    Le chien court dans le jardin  [Le, chien, court, dans, le, jardin]\n",
            "3  Mangeons des pommes delicieuses  [Mangeons, des, pommes, delicieuses]\n",
            "4      Je mange une orange fraiche     [Je, mange, une, orange, fraiche]\n"
          ]
        }
      ],
      "source": [
        "# Fonction pour tokenizer le corpus dans la colonne \"t_s_p\"\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Appliquer la fonction de tokenization à la colonne \"t_s_p\"\n",
        "df['tokens'] = df['t_s_p'].apply(tokenize_text)\n",
        "\n",
        "# Afficher le DataFrame avec la colonne \"tokens\"\n",
        "print(df[['Texte', 'tokens']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueylgUCzfRLv"
      },
      "source": [
        "7. Ecrire une fonction qui élimine les stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "balfXlCHfRLv",
        "outputId": "afd43f76-992c-4600-dc73-3a28ff38d933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['chat', 'dort', 'tapis']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Assurez-vous de télécharger les stop words si vous ne les avez pas déjà téléchargés\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# Liste des stop words en français\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "# Fonction pour éliminer les stop words d'une liste de tokens\n",
        "def remove_stopwords(tokens):\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Exemple d'utilisation de la fonction\n",
        "tokens = [\"Le\", \"chat\", \"dort\", \"sur\", \"le\", \"tapis\"]\n",
        "tokens_without_stopwords = remove_stopwords(tokens)\n",
        "print(tokens_without_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGckcM8ifRL0",
        "outputId": "ca123051-fafe-4205-f0cf-c7255430d687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             Texte         tokens_without_stopwords\n",
            "0        Le chat dort sur le tapis              [chat, dort, tapis]\n",
            "1    Les Oiseaux Chantent Le Matin       [Oiseaux, Chantent, Matin]\n",
            "2    Le chien court dans le jardin           [chien, court, jardin]\n",
            "3  Mangeons des pommes delicieuses  [Mangeons, pommes, delicieuses]\n",
            "4      Je mange une orange fraiche         [mange, orange, fraiche]\n"
          ]
        }
      ],
      "source": [
        "# Appliquer la fonction pour éliminer les stop words à la colonne \"tokens\"\n",
        "df['tokens_without_stopwords'] = df['tokens'].apply(remove_stopwords)\n",
        "\n",
        "# Afficher le DataFrame avec la colonne \"tokens_without_stopwords\"\n",
        "print(df[['Texte', 'tokens_without_stopwords']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMmzujKfRL1"
      },
      "source": [
        "8. Appliquer la lemmatisation et stremming sur le corpus sans stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-LBMviJfRL1",
        "outputId": "aafc0b3e-3225-4b12-ece2-c08bf6cb3cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens d'origine: ['mange', 'des', 'pommes', 'délicieuses']\n",
            "Tokens sans stop words: ['mange', 'pommes', 'délicieuses']\n",
            "Lemmatisation: ['mange', 'pommes', 'délicieuses']\n",
            "Stemming: ['mang', 'pomm', 'délici']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# Assurez-vous de télécharger les données nécessaires\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "# Liste des stop words en français\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "# Fonction pour éliminer les stop words d'une liste de tokens\n",
        "def remove_stopwords(tokens):\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Fonction pour lemmatiser une liste de tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Fonction pour effectuer le stemming sur une liste de tokens\n",
        "def stem_tokens(tokens):\n",
        "    stemmer = SnowballStemmer('french')\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Exemple d'utilisation des fonctions\n",
        "tokens = [\"mange\", \"des\", \"pommes\", \"délicieuses\"]\n",
        "tokens_without_stopwords = remove_stopwords(tokens)\n",
        "lemmatized_tokens = lemmatize_tokens(tokens_without_stopwords)\n",
        "stemmed_tokens = stem_tokens(tokens_without_stopwords)\n",
        "\n",
        "print(\"Tokens d'origine:\", tokens)\n",
        "print(\"Tokens sans stop words:\", tokens_without_stopwords)\n",
        "print(\"Lemmatisation:\", lemmatized_tokens)\n",
        "print(\"Stemming:\", stemmed_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e98jqaNfRL2"
      },
      "source": [
        "Partie 2 : CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-uq5fqifRL2"
      },
      "source": [
        "9. Initialiser et ajuster le CountVectorizer à votre corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY5s4KgsfRL3",
        "outputId": "e7cfedef-abaf-42fa-b101-25397acbf0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   chantent  chat  chien  court  dans  des  dort  délicieuses  fraiche  \\\n",
            "0         0     1      0      0     0    0     1            0        0   \n",
            "1         1     0      0      0     0    0     0            0        0   \n",
            "2         0     0      1      1     1    0     0            0        0   \n",
            "3         0     0      0      0     0    1     0            1        0   \n",
            "4         0     0      0      0     0    0     0            0        1   \n",
            "\n",
            "   jardin  ...  les  mange  mangeons  matin  oiseaux  orange  pommes  sur  \\\n",
            "0       0  ...    0      0         0      0        0       0       0    1   \n",
            "1       0  ...    1      0         0      1        1       0       0    0   \n",
            "2       1  ...    0      0         0      0        0       0       0    0   \n",
            "3       0  ...    0      0         1      0        0       0       1    0   \n",
            "4       0  ...    0      1         0      0        0       1       0    0   \n",
            "\n",
            "   tapis  une  \n",
            "0      1    0  \n",
            "1      0    0  \n",
            "2      0    0  \n",
            "3      0    0  \n",
            "4      0    1  \n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Initialiser le CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Ajuster le CountVectorizer au corpus\n",
        "X = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Obtenez la matrice de comptage des mots\n",
        "count_matrix = X.toarray()\n",
        "\n",
        "# Obtenez les noms des fonctionnalités (mots)\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Créez un DataFrame pour afficher la matrice de comptage des mots\n",
        "import pandas as pd\n",
        "count_df = pd.DataFrame(data=count_matrix, columns=feature_names)\n",
        "\n",
        "# Affichez le DataFrame\n",
        "print(count_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ZF1m5_fRL3"
      },
      "source": [
        "10. Transformer le corpus en une matrice de comptage de tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qEA9D30fRL3",
        "outputId": "d8ec3ec5-a3eb-4514-d07b-dffa98de3a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   chantent  chat  chien  court  dans  des  dort  délicieuses  fraiche  \\\n",
            "0         0     1      0      0     0    0     1            0        0   \n",
            "1         1     0      0      0     0    0     0            0        0   \n",
            "2         0     0      1      1     1    0     0            0        0   \n",
            "3         0     0      0      0     0    1     0            1        0   \n",
            "4         0     0      0      0     0    0     0            0        1   \n",
            "\n",
            "   jardin  ...  les  mange  mangeons  matin  oiseaux  orange  pommes  sur  \\\n",
            "0       0  ...    0      0         0      0        0       0       0    1   \n",
            "1       0  ...    1      0         0      1        1       0       0    0   \n",
            "2       1  ...    0      0         0      0        0       0       0    0   \n",
            "3       0  ...    0      0         1      0        0       0       1    0   \n",
            "4       0  ...    0      1         0      0        0       1       0    0   \n",
            "\n",
            "   tapis  une  \n",
            "0      1    0  \n",
            "1      0    0  \n",
            "2      0    0  \n",
            "3      0    0  \n",
            "4      0    1  \n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Créez un DataFrame pour afficher la matrice de comptage des mots\n",
        "count_df = pd.DataFrame(data=count_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Affichez le DataFrame\n",
        "print(count_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmdSM5vdfRL4"
      },
      "source": [
        "11. Explorer la matrice résultante pour comprendre comment les tokens sont représentés en\n",
        "vecteurs binaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-qHW5PGfRL4",
        "outputId": "d295a6d2-d40d-4714-c027-109d4e36ffa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   chantent  chat  chien  court  dans  des  dort  délicieuses  fraiche  \\\n",
            "0         0     1      0      0     0    0     1            0        0   \n",
            "1         1     0      0      0     0    0     0            0        0   \n",
            "2         0     0      1      1     1    0     0            0        0   \n",
            "3         0     0      0      0     0    1     0            1        0   \n",
            "4         0     0      0      0     0    0     0            0        1   \n",
            "\n",
            "   jardin  ...  les  mange  mangeons  matin  oiseaux  orange  pommes  sur  \\\n",
            "0       0  ...    0      0         0      0        0       0       0    1   \n",
            "1       0  ...    1      0         0      1        1       0       0    0   \n",
            "2       1  ...    0      0         0      0        0       0       0    0   \n",
            "3       0  ...    0      0         1      0        0       0       1    0   \n",
            "4       0  ...    0      1         0      0        0       1       0    0   \n",
            "\n",
            "   tapis  une  \n",
            "0      1    0  \n",
            "1      0    0  \n",
            "2      0    0  \n",
            "3      0    0  \n",
            "4      0    1  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "Noms de fonctionnalités (tokens) :\n",
            "['chantent' 'chat' 'chien' 'court' 'dans' 'des' 'dort' 'délicieuses'\n",
            " 'fraiche' 'jardin' 'je' 'le' 'les' 'mange' 'mangeons' 'matin' 'oiseaux'\n",
            " 'orange' 'pommes' 'sur' 'tapis' 'une']\n",
            "Comptage des tokens pour le document 'Le chat dort sur le tapis':\n",
            "chantent       0\n",
            "chat           1\n",
            "chien          0\n",
            "court          0\n",
            "dans           0\n",
            "des            0\n",
            "dort           1\n",
            "délicieuses    0\n",
            "fraiche        0\n",
            "jardin         0\n",
            "je             0\n",
            "le             2\n",
            "les            0\n",
            "mange          0\n",
            "mangeons       0\n",
            "matin          0\n",
            "oiseaux        0\n",
            "orange         0\n",
            "pommes         0\n",
            "sur            1\n",
            "tapis          1\n",
            "une            0\n",
            "Name: 0, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Votre code précédent pour créer la matrice de comptage des tokens\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform(corpus)\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "count_df = pd.DataFrame(data=count_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Afficher les 5 premières lignes de la matrice\n",
        "print(count_df.head())\n",
        "\n",
        "# Afficher les noms de fonctionnalités (tokens)\n",
        "print(\"Noms de fonctionnalités (tokens) :\")\n",
        "print(feature_names)\n",
        "\n",
        "# Afficher le comptage pour un document spécifique\n",
        "document_index = 0\n",
        "document_name = corpus[document_index]\n",
        "document_counts = count_df.iloc[document_index]\n",
        "print(f\"Comptage des tokens pour le document '{document_name}':\")\n",
        "print(document_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmG7lCMXfRL5"
      },
      "source": [
        "Partie 3 : TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSx5rcOtfRL5"
      },
      "source": [
        "1. Initialiser et ajuster le TfidfVectorizer à votre corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK92UFdUfRL5",
        "outputId": "1d818706-2366-4893-9290-ecd0ed655a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   chantent     chat    chien    court     dans  des     dort  délicieuses  \\\n",
            "0  0.000000  0.41544  0.00000  0.00000  0.00000  0.0  0.41544          0.0   \n",
            "1  0.474125  0.00000  0.00000  0.00000  0.00000  0.0  0.00000          0.0   \n",
            "2  0.000000  0.00000  0.41544  0.41544  0.41544  0.0  0.00000          0.0   \n",
            "3  0.000000  0.00000  0.00000  0.00000  0.00000  0.5  0.00000          0.5   \n",
            "4  0.000000  0.00000  0.00000  0.00000  0.00000  0.0  0.00000          0.0   \n",
            "\n",
            "    fraiche   jardin  ...       les     mange  mangeons     matin   oiseaux  \\\n",
            "0  0.000000  0.00000  ...  0.000000  0.000000       0.0  0.000000  0.000000   \n",
            "1  0.000000  0.00000  ...  0.474125  0.000000       0.0  0.474125  0.474125   \n",
            "2  0.000000  0.41544  ...  0.000000  0.000000       0.0  0.000000  0.000000   \n",
            "3  0.000000  0.00000  ...  0.000000  0.000000       0.5  0.000000  0.000000   \n",
            "4  0.447214  0.00000  ...  0.000000  0.447214       0.0  0.000000  0.000000   \n",
            "\n",
            "     orange  pommes      sur    tapis       une  \n",
            "0  0.000000     0.0  0.41544  0.41544  0.000000  \n",
            "1  0.000000     0.0  0.00000  0.00000  0.000000  \n",
            "2  0.000000     0.0  0.00000  0.00000  0.000000  \n",
            "3  0.000000     0.5  0.00000  0.00000  0.000000  \n",
            "4  0.447214     0.0  0.00000  0.00000  0.447214  \n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Initialiser le TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Ajuster le TfidfVectorizer au corpus\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Obtenez les noms des fonctionnalités (mots)\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Créez un DataFrame pour afficher la matrice TF-IDF\n",
        "import pandas as pd\n",
        "tfidf_df = pd.DataFrame(data=X_tfidf.toarray(), columns=feature_names_tfidf)\n",
        "\n",
        "# Affichez le DataFrame\n",
        "print(tfidf_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aczTCBD3fRL6"
      },
      "source": [
        "2. Transformer le corpus en une matrice de poids de tokens dans le corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7DlqpT7fRL6",
        "outputId": "2640ad5f-4676-4669-df3a-c27ae0ecb2d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   chantent     chat    chien    court     dans  des     dort  délicieuses  \\\n",
            "0  0.000000  0.41544  0.00000  0.00000  0.00000  0.0  0.41544          0.0   \n",
            "1  0.474125  0.00000  0.00000  0.00000  0.00000  0.0  0.00000          0.0   \n",
            "2  0.000000  0.00000  0.41544  0.41544  0.41544  0.0  0.00000          0.0   \n",
            "3  0.000000  0.00000  0.00000  0.00000  0.00000  0.5  0.00000          0.5   \n",
            "4  0.000000  0.00000  0.00000  0.00000  0.00000  0.0  0.00000          0.0   \n",
            "\n",
            "    fraiche   jardin  ...       les     mange  mangeons     matin   oiseaux  \\\n",
            "0  0.000000  0.00000  ...  0.000000  0.000000       0.0  0.000000  0.000000   \n",
            "1  0.000000  0.00000  ...  0.474125  0.000000       0.0  0.474125  0.474125   \n",
            "2  0.000000  0.41544  ...  0.000000  0.000000       0.0  0.000000  0.000000   \n",
            "3  0.000000  0.00000  ...  0.000000  0.000000       0.5  0.000000  0.000000   \n",
            "4  0.447214  0.00000  ...  0.000000  0.447214       0.0  0.000000  0.000000   \n",
            "\n",
            "     orange  pommes      sur    tapis       une  \n",
            "0  0.000000     0.0  0.41544  0.41544  0.000000  \n",
            "1  0.000000     0.0  0.00000  0.00000  0.000000  \n",
            "2  0.000000     0.0  0.00000  0.00000  0.000000  \n",
            "3  0.000000     0.5  0.00000  0.00000  0.000000  \n",
            "4  0.447214     0.0  0.00000  0.00000  0.447214  \n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Initialiser le TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Ajuster le TfidfVectorizer au corpus et transformer en matrice de poids TF-IDF\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Obtenez les noms des fonctionnalités (tokens)\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Créez un DataFrame pour afficher la matrice de poids TF-IDF\n",
        "import pandas as pd\n",
        "tfidf_df = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names_tfidf)\n",
        "\n",
        "# Affichez le DataFrame\n",
        "print(tfidf_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAOjghvHfRL7"
      },
      "source": [
        "3. Explorer la matrice résultante pour comprendre comment les tokens sont représentés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-GwmFYNfRL7",
        "outputId": "8a73061e-5f27-427c-e730-128e56358936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   chantent     chat    chien    court     dans  des     dort  délicieuses  \\\n",
            "0  0.000000  0.41544  0.00000  0.00000  0.00000  0.0  0.41544          0.0   \n",
            "1  0.474125  0.00000  0.00000  0.00000  0.00000  0.0  0.00000          0.0   \n",
            "2  0.000000  0.00000  0.41544  0.41544  0.41544  0.0  0.00000          0.0   \n",
            "3  0.000000  0.00000  0.00000  0.00000  0.00000  0.5  0.00000          0.5   \n",
            "4  0.000000  0.00000  0.00000  0.00000  0.00000  0.0  0.00000          0.0   \n",
            "\n",
            "    fraiche   jardin  ...       les     mange  mangeons     matin   oiseaux  \\\n",
            "0  0.000000  0.00000  ...  0.000000  0.000000       0.0  0.000000  0.000000   \n",
            "1  0.000000  0.00000  ...  0.474125  0.000000       0.0  0.474125  0.474125   \n",
            "2  0.000000  0.41544  ...  0.000000  0.000000       0.0  0.000000  0.000000   \n",
            "3  0.000000  0.00000  ...  0.000000  0.000000       0.5  0.000000  0.000000   \n",
            "4  0.447214  0.00000  ...  0.000000  0.447214       0.0  0.000000  0.000000   \n",
            "\n",
            "     orange  pommes      sur    tapis       une  \n",
            "0  0.000000     0.0  0.41544  0.41544  0.000000  \n",
            "1  0.000000     0.0  0.00000  0.00000  0.000000  \n",
            "2  0.000000     0.0  0.00000  0.00000  0.000000  \n",
            "3  0.000000     0.5  0.00000  0.00000  0.000000  \n",
            "4  0.447214     0.0  0.00000  0.00000  0.447214  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "Noms de fonctionnalités (tokens) :\n",
            "['chantent' 'chat' 'chien' 'court' 'dans' 'des' 'dort' 'délicieuses'\n",
            " 'fraiche' 'jardin' 'je' 'le' 'les' 'mange' 'mangeons' 'matin' 'oiseaux'\n",
            " 'orange' 'pommes' 'sur' 'tapis' 'une']\n",
            "Poids TF-IDF pour le document 'Le chat dort sur le tapis':\n",
            "chantent       0.000000\n",
            "chat           0.415440\n",
            "chien          0.000000\n",
            "court          0.000000\n",
            "dans           0.000000\n",
            "des            0.000000\n",
            "dort           0.415440\n",
            "délicieuses    0.000000\n",
            "fraiche        0.000000\n",
            "jardin         0.000000\n",
            "je             0.000000\n",
            "le             0.556451\n",
            "les            0.000000\n",
            "mange          0.000000\n",
            "mangeons       0.000000\n",
            "matin          0.000000\n",
            "oiseaux        0.000000\n",
            "orange         0.000000\n",
            "pommes         0.000000\n",
            "sur            0.415440\n",
            "tapis          0.415440\n",
            "une            0.000000\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# créer la matrice de poids TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "tfidf_df = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names_tfidf)\n",
        "\n",
        "# Afficher les 5 premières lignes de la matrice de poids TF-IDF\n",
        "print(tfidf_df.head())\n",
        "\n",
        "# Afficher les noms de fonctionnalités (tokens)\n",
        "print(\"Noms de fonctionnalités (tokens) :\")\n",
        "print(feature_names_tfidf)\n",
        "\n",
        "# Afficher les poids TF-IDF pour un document spécifique\n",
        "document_index = 0\n",
        "document_name = corpus[document_index]\n",
        "document_tfidf = tfidf_df.iloc[document_index]\n",
        "print(f\"Poids TF-IDF pour le document '{document_name}':\")\n",
        "print(document_tfidf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo49FZ0MfRL7"
      },
      "source": [
        "4. Appliquer la métrique similarité de cosinus entre les tokens « chat et chien », puis après\n",
        "« pomme et orange » sur les deux représentations vectorielles. Conclure ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKXxDom9fRL8",
        "outputId": "8d3c5d9e-653a-4caa-fc6d-6d3906215e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarité de cosinus entre 'chat' et 'chien' : 0.0\n",
            "Similarité de cosinus entre 'pomme' et 'orange' : 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Obtenez les noms de fonctionnalités (tokens) et leurs indices\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Recherchez les indices des mots dans le vocabulaire du TfidfVectorizer\n",
        "index_chat = feature_names_tfidf.tolist().index(\"chat\")\n",
        "index_chien = feature_names_tfidf.tolist().index(\"chien\")\n",
        "index_pomme = feature_names_tfidf.tolist().index(\"pommes\")\n",
        "index_orange = feature_names_tfidf.tolist().index(\"orange\")\n",
        "\n",
        "# Extraire les vecteurs TF-IDF correspondants aux mots\n",
        "vector_chat = tfidf_matrix[:, index_chat].reshape(1, -1)\n",
        "vector_chien = tfidf_matrix[:, index_chien].reshape(1, -1)\n",
        "vector_pomme = tfidf_matrix[:, index_pomme].reshape(1, -1)\n",
        "vector_orange = tfidf_matrix[:, index_orange].reshape(1, -1)\n",
        "\n",
        "# Calculer la similarité de cosinus entre les vecteurs\n",
        "similarite_chat_chien = cosine_similarity(vector_chat, vector_chien)[0][0]\n",
        "similarite_pomme_orange = cosine_similarity(vector_pomme, vector_orange)[0][0]\n",
        "\n",
        "\n",
        "# Afficher la similarité\n",
        "print(\"Similarité de cosinus entre 'chat' et 'chien' :\", similarite_chat_chien)\n",
        "print(\"Similarité de cosinus entre 'pomme' et 'orange' :\", similarite_pomme_orange)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgd2BBkPfRL8"
      },
      "source": [
        "3. Quelles sont les différentes approches pour prétraiter un corpus de texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp2Bx7PxfRL8"
      },
      "source": [
        "Chargement du dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugmn3hcGfRL9",
        "outputId": "eb717678-968e-48a3-901c-027f24184779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    COMMENT_ID            AUTHOR  \\\n",
            "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
            "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
            "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
            "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
            "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
            "\n",
            "                  DATE                                            CONTENT  \\\n",
            "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
            "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
            "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
            "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
            "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
            "\n",
            "   CLASS  \n",
            "0      1  \n",
            "1      1  \n",
            "2      1  \n",
            "3      1  \n",
            "4      1  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Chargement du dataset (remplacez le chemin par le chemin de votre fichier CSV)\n",
        "dataset_path = \"youtube+spam+collection/Youtube01-Psy.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Affichez les premières lignes pour vérifier le chargement\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi5MoXQFfRMJ"
      },
      "source": [
        "Nettoyage du texte :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTNMVts6fRMJ",
        "outputId": "01642e7f-6f75-4971-b7b8-b29311bdf5da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             CONTENT  \\\n",
            "0  Huh, anyway check out this you[tube] channel: ...   \n",
            "1  Hey guys check out my new channel and our firs...   \n",
            "2             just for test I have to say murdev.com   \n",
            "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
            "4            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
            "\n",
            "                                     CLEANED_CONTENT  \n",
            "0  huh anyway check out this youtube channel koby...  \n",
            "1  hey guys check out my new channel and our firs...  \n",
            "2              just for test i have to say murdevcom  \n",
            "3      me shaking my sexy ass on my channel enjoy  ﻿  \n",
            "4               watchvvtarggvgtwq   check this out ﻿  \n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # Supprimer les caractères spéciaux\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    # Convertir en minuscules\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Appliquer la fonction de nettoyage à la colonne \"CONTENT\"\n",
        "df['CLEANED_CONTENT'] = df['CONTENT'].apply(clean_text)\n",
        "\n",
        "# Affichez les premières lignes pour vérifier le nettoyage\n",
        "print(df[['CONTENT', 'CLEANED_CONTENT']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLNsmRktfRMJ"
      },
      "source": [
        "Tokenization :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7LJ6Ij3fRMK",
        "outputId": "aca32315-3df1-45c6-a3d2-345919e6b2c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     CLEANED_CONTENT  \\\n",
            "0  huh anyway check out this youtube channel koby...   \n",
            "1  hey guys check out my new channel and our firs...   \n",
            "2              just for test i have to say murdevcom   \n",
            "3      me shaking my sexy ass on my channel enjoy  ﻿   \n",
            "4               watchvvtarggvgtwq   check this out ﻿   \n",
            "\n",
            "                                   TOKENIZED_CONTENT  \n",
            "0  [huh, anyway, check, out, this, youtube, chann...  \n",
            "1  [hey, guys, check, out, my, new, channel, and,...  \n",
            "2     [just, for, test, i, have, to, say, murdevcom]  \n",
            "3  [me, shaking, my, sexy, ass, on, my, channel, ...  \n",
            "4           [watchvvtarggvgtwq, check, this, out, ﻿]  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Assurez-vous d'avoir téléchargé les données NLTK (seulement une fois)\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Appliquer la tokenization à la colonne \"CLEANED_CONTENT\"\n",
        "df['TOKENIZED_CONTENT'] = df['CLEANED_CONTENT'].apply(tokenize_text)\n",
        "\n",
        "# Affichez les premières lignes pour vérifier la tokenization\n",
        "print(df[['CLEANED_CONTENT', 'TOKENIZED_CONTENT']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCNisd_0fRMK"
      },
      "source": [
        "Élimination des Stop Words :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvroe6aofRMK",
        "outputId": "4c5a5a63-3b59-44da-97f2-e20e28d108d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                   TOKENIZED_CONTENT  \\\n",
            "0  [huh, anyway, check, out, this, youtube, chann...   \n",
            "1  [hey, guys, check, out, my, new, channel, and,...   \n",
            "2     [just, for, test, i, have, to, say, murdevcom]   \n",
            "3  [me, shaking, my, sexy, ass, on, my, channel, ...   \n",
            "4           [watchvvtarggvgtwq, check, this, out, ﻿]   \n",
            "\n",
            "                                    FILTERED_CONTENT  \n",
            "0  [huh, anyway, check, youtube, channel, kobyosh...  \n",
            "1  [hey, guys, check, new, channel, first, vid, u...  \n",
            "2                             [test, say, murdevcom]  \n",
            "3            [shaking, sexy, ass, channel, enjoy, ﻿]  \n",
            "4                      [watchvvtarggvgtwq, check, ﻿]  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Assurez-vous d'avoir téléchargé les stop words NLTK (seulement une fois)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))  # Vous pouvez également spécifier une autre langue si nécessaire\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Appliquer l'élimination des stop words à la colonne \"TOKENIZED_CONTENT\"\n",
        "df['FILTERED_CONTENT'] = df['TOKENIZED_CONTENT'].apply(remove_stop_words)\n",
        "\n",
        "# Affichez les premières lignes pour vérifier l'élimination des stop words\n",
        "print(df[['TOKENIZED_CONTENT', 'FILTERED_CONTENT']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-D9rnW_fRMK"
      },
      "source": [
        "Lemmatisation (en utilisant NLTK) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HOfxG2VfRMK",
        "outputId": "072b4bf6-3198-493e-b69c-e73795ff3a0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    FILTERED_CONTENT  \\\n",
            "0  [huh, anyway, check, youtube, channel, kobyosh...   \n",
            "1  [hey, guys, check, new, channel, first, vid, u...   \n",
            "2                             [test, say, murdevcom]   \n",
            "3            [shaking, sexy, ass, channel, enjoy, ﻿]   \n",
            "4                      [watchvvtarggvgtwq, check, ﻿]   \n",
            "\n",
            "                                  LEMMATIZED_CONTENT  \n",
            "0  [huh, anyway, check, youtube, channel, kobyosh...  \n",
            "1  [hey, guy, check, new, channel, first, vid, u,...  \n",
            "2                             [test, say, murdevcom]  \n",
            "3             [shaking, sexy, as, channel, enjoy, ﻿]  \n",
            "4                      [watchvvtarggvgtwq, check, ﻿]  \n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Appliquer la lemmatisation à la colonne \"FILTERED_CONTENT\"\n",
        "df['LEMMATIZED_CONTENT'] = df['FILTERED_CONTENT'].apply(lemmatize_tokens)\n",
        "\n",
        "# Affichez les premières lignes pour vérifier la lemmatisation\n",
        "print(df[['FILTERED_CONTENT', 'LEMMATIZED_CONTENT']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aTUC_55fRML"
      },
      "source": [
        "Stemming (en utilisant NLTK) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUxZkXBffRML",
        "outputId": "851cc344-170d-4e10-c161-e495cfccceee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    FILTERED_CONTENT  \\\n",
            "0  [huh, anyway, check, youtube, channel, kobyosh...   \n",
            "1  [hey, guys, check, new, channel, first, vid, u...   \n",
            "2                             [test, say, murdevcom]   \n",
            "3            [shaking, sexy, ass, channel, enjoy, ﻿]   \n",
            "4                      [watchvvtarggvgtwq, check, ﻿]   \n",
            "\n",
            "                                     STEMMED_CONTENT  \n",
            "0  [huh, anyway, check, youtub, channel, kobyoshi02]  \n",
            "1  [hey, guy, check, new, channel, first, vid, us...  \n",
            "2                             [test, say, murdevcom]  \n",
            "3              [shake, sexi, ass, channel, enjoy, ﻿]  \n",
            "4                      [watchvvtarggvgtwq, check, ﻿]  \n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Appliquer le stemming à la colonne \"FILTERED_CONTENT\"\n",
        "df['STEMMED_CONTENT'] = df['FILTERED_CONTENT'].apply(stem_tokens)\n",
        "\n",
        "# Affichez les premières lignes pour vérifier le stemming\n",
        "print(df[['FILTERED_CONTENT', 'STEMMED_CONTENT']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mLqIatjfRML"
      },
      "source": [
        "Vectorisation avec le TfidfVectorizer :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ0zam9rfRML",
        "outputId": "42257175-264a-4ccf-f476-3e67b7309417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   002  0058   10  100  1000000  1030   11  111   12  139  ...  youtuber   yr  \\\n",
            "0  0.0   0.0  0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0  0.0   \n",
            "1  0.0   0.0  0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0  0.0   \n",
            "2  0.0   0.0  0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0  0.0   \n",
            "3  0.0   0.0  0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0  0.0   \n",
            "4  0.0   0.0  0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  ...       0.0  0.0   \n",
            "\n",
            "    yt  zero  zombie  강남스타일  ｃｏｍｍｅｎｔ  ｄａｍｎ  ｆａｎｃy   ｉｓ  \n",
            "0  0.0   0.0     0.0    0.0      0.0   0.0    0.0  0.0  \n",
            "1  0.0   0.0     0.0    0.0      0.0   0.0    0.0  0.0  \n",
            "2  0.0   0.0     0.0    0.0      0.0   0.0    0.0  0.0  \n",
            "3  0.0   0.0     0.0    0.0      0.0   0.0    0.0  0.0  \n",
            "4  0.0   0.0     0.0    0.0      0.0   0.0    0.0  0.0  \n",
            "\n",
            "[5 rows x 1000 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Créer une instance du TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Vous pouvez ajuster le nombre maximal de fonctionnalités (mots) si nécessaire\n",
        "# Convertir la liste de tokens en une chaîne de texte pour chaque commentaire\n",
        "df['LEMMATIZED_CONTENT'] = df['LEMMATIZED_CONTENT'].apply(lambda tokens: ' '.join(tokens))\n",
        "# Adapter le vectoriseur sur les données filtrées (après élimination des stop words)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['LEMMATIZED_CONTENT'])  # Utilisez 'STEMMED_CONTENT' si vous avez choisi le stemming\n",
        "\n",
        "# Convertir la matrice TF-IDF en un DataFrame pandas (facultatif, mais utile pour l'inspection)\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Affichez les premières lignes de la matrice TF-IDF (si nécessaire)\n",
        "print(tfidf_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9st9iScfRML"
      },
      "source": [
        "Préparation pour la Classification (facultatif) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7AmJUQGfRMM",
        "outputId": "097dc345-21ae-4050-f636-fb9545552bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                  LEMMATIZED_CONTENT  LABEL\n",
            "0        huh anyway check youtube channel kobyoshi02      1\n",
            "1  hey guy check new channel first vid u monkey i...      1\n",
            "2                                 test say murdevcom      1\n",
            "3                    shaking sexy as channel enjoy ﻿      1\n",
            "4                          watchvvtarggvgtwq check ﻿      1\n"
          ]
        }
      ],
      "source": [
        "# Ajouter une colonne d'étiquettes au DataFrame (1 pour spam, 0 pour non-spam)\n",
        "df['LABEL'] = df['CLASS']\n",
        "\n",
        "# Affichez les premières lignes pour vérifier les étiquettes\n",
        "print(df[['LEMMATIZED_CONTENT', 'LABEL']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdHo3UJfRMM"
      },
      "source": [
        "Classification (facultatif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJHi1AnUfRMM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression  # Vous pouvez choisir un autre algorithme de classification\n",
        "\n",
        "# Divisez les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['LABEL'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Créez un modèle de classification (par exemple, régression logistique)\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Entraînez le modèle sur les données d'entraînement\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Prédisez les étiquettes sur les données de test\n",
        "y_pred = classifier.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qLZFWxVfRMM"
      },
      "source": [
        "Évaluation du Modèle :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzs68lDwfRMN",
        "outputId": "68fb9d29-c6d8-4e14-b188-ad0365d21c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.96      0.81        27\n",
            "           1       0.97      0.74      0.84        43\n",
            "\n",
            "    accuracy                           0.83        70\n",
            "   macro avg       0.84      0.85      0.83        70\n",
            "weighted avg       0.87      0.83      0.83        70\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26  1]\n",
            " [11 32]]\n",
            "Accuracy: 0.8285714285714286\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Évaluez le modèle\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfHIGsNWfRMN"
      },
      "source": [
        "Analyse des Résultats : Enfin, analysez les résultats pour comprendre quelles caractéristiques (mots) sont importantes pour détecter le spam et examinez les erreurs de classification pour voir s'il y a des tendances ou des motifs spécifiques.\n",
        "C'est la dernière étape du processus de prétraitement et de classification des données textuelles. Si vous avez d'autres questions, des besoins spécifiques ou si vous souhaitez explorer davantage certains aspects, n'hésitez pas à me le faire savoir. Je suis là pour vous aider."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}